"""
Generated Flow Executor - DSL-Driven E2E Tests (v0.13.0)

This test file executes flows generated by the DAZZLE testspec generator.
Unlike the generic test_ux_validation.py, these tests are:
- Generated from DSL specs (entities, state machines, access control, etc.)
- Comprehensive (CRUD, validation, navigation, state transitions, computed fields)
- Categorized by flow type for better reporting

Flow Types Generated:
- crud: Create, Read, Update, Delete operations
- validation: Field constraint tests
- navigation: Surface/view navigation
- state_machine: Valid/invalid state transitions
- computed: Computed field verification
- access_control: Permission granted/denied tests
- reference: Foreign key integrity tests

Usage (inside Docker):
    pytest test_generated_flows.py -v --screenshot=on

Environment variables:
    DNR_BASE_URL: API URL (default: http://dnr-app:8000)
    DNR_UI_URL: UI URL (default: http://dnr-app:3000)
    SCREENSHOT_DIR: Where to save screenshots (default: /screenshots)
    EXAMPLE_NAME: Name of the example being tested
"""

import json
import os
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any

import httpx
import pytest
from playwright.sync_api import Page, expect

# Add src to path for dazzle imports
sys.path.insert(0, "/src")

# Configuration from environment
DNR_BASE_URL = os.environ.get("DNR_BASE_URL", "http://localhost:8000")
DNR_UI_URL = os.environ.get("DNR_UI_URL", DNR_BASE_URL.replace(":8000", ":3000"))
SCREENSHOT_DIR = os.environ.get("SCREENSHOT_DIR", "/screenshots")
EXAMPLE_NAME = os.environ.get("EXAMPLE_NAME", "unknown")


# =============================================================================
# Flow Execution Engine
# =============================================================================


@dataclass
class FlowResult:
    """Result of executing a single flow."""

    flow_id: str
    flow_type: str
    passed: bool
    error: str | None = None
    screenshot: str | None = None


class FlowExecutor:
    """Execute generated E2E flows against the running app."""

    def __init__(self, page: Page, api_client: httpx.Client, ux_tracker: Any):
        self.page = page
        self.api_client = api_client
        self.ux_tracker = ux_tracker
        self.created_entities: dict[str, list[str]] = {}  # entity -> [ids]

    def execute_flow(self, flow: dict) -> FlowResult:
        """Execute a single flow and return the result."""
        flow_id = flow.get("id", "unknown")
        flow_type = self._get_flow_type(flow)

        try:
            # Execute each step in the flow
            for step in flow.get("steps", []):
                self._execute_step(step, flow)

            # Execute assertions
            for assertion in flow.get("assertions", []):
                self._execute_assertion(assertion, flow)

            # Track coverage
            self._track_coverage(flow)

            return FlowResult(
                flow_id=flow_id,
                flow_type=flow_type,
                passed=True,
                screenshot=self._take_screenshot(flow_id),
            )

        except Exception as e:
            return FlowResult(
                flow_id=flow_id,
                flow_type=flow_type,
                passed=False,
                error=str(e),
                screenshot=self._take_screenshot(f"{flow_id}_error"),
            )

    def _get_flow_type(self, flow: dict) -> str:
        """Determine flow type from tags."""
        tags = flow.get("tags", [])
        for tag in [
            "crud",
            "validation",
            "navigation",
            "state_machine",
            "computed",
            "access_control",
            "reference",
        ]:
            if tag in tags:
                return tag
        return "other"

    def _execute_step(self, step: dict, flow: dict) -> None:
        """Execute a single step."""
        kind = step.get("kind", "")
        target = step.get("target", "")
        data = step.get("data", {})

        if kind == "navigate":
            self._navigate(target)
        elif kind == "click":
            self._click(target)
        elif kind == "fill":
            self._fill_form(data)
        elif kind == "submit":
            self._submit_form()
        elif kind == "api_call":
            self._api_call(step, flow)
        elif kind == "wait":
            self.page.wait_for_timeout(int(data.get("ms", 1000)))
        elif kind == "login":
            self._login(data.get("persona", "default"))

    def _navigate(self, target: str) -> None:
        """Navigate to a route."""
        if target.startswith("/"):
            url = f"{DNR_UI_URL}{target}"
        else:
            url = f"{DNR_UI_URL}/{target}"
        self.page.goto(url)
        self.page.wait_for_load_state("networkidle")

    def _click(self, target: str) -> None:
        """Click an element using semantic selectors."""
        # Try semantic selectors first
        selectors = [
            f"[data-dazzle-action='{target}']",
            f"[data-dazzle-entity='{target}']",
            f"button:has-text('{target}')",
            f"a:has-text('{target}')",
            f"[data-action='{target}']",
            target,  # Raw selector as fallback
        ]

        for selector in selectors:
            try:
                element = self.page.locator(selector).first
                if element.count() > 0:
                    element.click()
                    self.page.wait_for_timeout(500)
                    return
            except Exception:
                continue

        raise ValueError(f"Could not find element to click: {target}")

    def _fill_form(self, data: dict) -> None:
        """Fill form fields."""
        for field_name, value in data.items():
            # Try semantic selectors
            selectors = [
                f"[data-dazzle-field='{field_name}']",
                f"[name='{field_name}']",
                f"#field-{field_name}",
                f"input[placeholder*='{field_name}' i]",
            ]

            for selector in selectors:
                try:
                    element = self.page.locator(selector).first
                    if element.count() > 0:
                        element.fill(str(value))
                        break
                except Exception:
                    continue

    def _submit_form(self) -> None:
        """Submit the current form."""
        submit_selectors = [
            "[data-dazzle-action='submit']",
            "button[type='submit']",
            "button:has-text('Save')",
            "button:has-text('Create')",
            "button:has-text('Submit')",
        ]

        for selector in submit_selectors:
            try:
                element = self.page.locator(selector).first
                if element.count() > 0:
                    element.click()
                    self.page.wait_for_timeout(1000)
                    return
            except Exception:
                continue

    def _api_call(self, step: dict, flow: dict) -> None:
        """Make an API call."""
        method = step.get("method", "GET").upper()
        endpoint = step.get("target", "")
        data = step.get("data", {})

        # Build endpoint URL
        if not endpoint.startswith("/"):
            endpoint = f"/api/{endpoint}"

        # Replace :id placeholder with actual ID if available
        entity_name = flow.get("entity", "")
        if ":id" in endpoint and entity_name in self.created_entities:
            ids = self.created_entities[entity_name]
            if ids:
                endpoint = endpoint.replace(":id", ids[-1])

        # Make the request
        if method == "GET":
            response = self.api_client.get(endpoint)
        elif method == "POST":
            response = self.api_client.post(endpoint, json=data)
            # Store created entity ID
            if response.status_code in (200, 201):
                result = response.json()
                if "id" in result:
                    if entity_name not in self.created_entities:
                        self.created_entities[entity_name] = []
                    self.created_entities[entity_name].append(str(result["id"]))
        elif method == "PUT":
            response = self.api_client.put(endpoint, json=data)
        elif method == "DELETE":
            response = self.api_client.delete(endpoint)
        else:
            raise ValueError(f"Unknown HTTP method: {method}")

        # Store response for assertions
        step["_response"] = response

    def _login(self, persona: str) -> None:
        """Login as a persona (placeholder for auth)."""
        # In DNR test mode, auth is typically disabled
        # This would be implemented based on the app's auth mechanism
        pass

    def _execute_assertion(self, assertion: dict, flow: dict) -> None:
        """Execute an assertion."""
        kind = assertion.get("kind", "")
        target = assertion.get("target", "")
        expected = assertion.get("expected")

        if kind == "visible":
            self._assert_visible(target)
        elif kind == "not_visible":
            self._assert_not_visible(target)
        elif kind == "text_contains":
            self._assert_text_contains(target, expected)
        elif kind == "api_status":
            self._assert_api_status(assertion, expected)
        elif kind == "count":
            self._assert_count(target, expected)
        elif kind == "state_transition_allowed":
            self._assert_state_allowed(flow, assertion)
        elif kind == "state_transition_blocked":
            self._assert_state_blocked(flow, assertion)
        elif kind == "computed_value":
            self._assert_computed_value(assertion, expected)
        elif kind == "permission_granted":
            self._assert_permission_granted(assertion)
        elif kind == "permission_denied":
            self._assert_permission_denied(assertion)
        elif kind == "ref_valid":
            self._assert_ref_valid(assertion)
        elif kind == "ref_invalid":
            self._assert_ref_invalid(assertion)

    def _assert_visible(self, target: str) -> None:
        """Assert element is visible."""
        selectors = [
            f"[data-dazzle-entity='{target}']",
            f"[data-dazzle-surface='{target}']",
            f":has-text('{target}')",
        ]

        for selector in selectors:
            try:
                element = self.page.locator(selector).first
                if element.count() > 0:
                    expect(element).to_be_visible(timeout=5000)
                    return
            except Exception:
                continue

        raise AssertionError(f"Element not visible: {target}")

    def _assert_not_visible(self, target: str) -> None:
        """Assert element is not visible."""
        selectors = [
            f"[data-dazzle-entity='{target}']",
            f":has-text('{target}')",
        ]

        for selector in selectors:
            try:
                element = self.page.locator(selector).first
                if element.count() > 0:
                    expect(element).not_to_be_visible(timeout=2000)
                    return
            except Exception:
                pass

    def _assert_text_contains(self, target: str, expected: str) -> None:
        """Assert element contains text."""
        body_text = self.page.locator("body").inner_text()
        assert expected in body_text, f"Expected '{expected}' not found in page"

    def _assert_api_status(self, assertion: dict, expected: int) -> None:
        """Assert API response status."""
        # This would check the last API call's status
        pass

    def _assert_count(self, target: str, expected: int) -> None:
        """Assert element count."""
        element = self.page.locator(f"[data-dazzle-entity='{target}']")
        assert element.count() >= expected, f"Expected at least {expected} elements"

    def _assert_state_allowed(self, flow: dict, assertion: dict) -> None:
        """Assert state transition was allowed."""
        # Check that the entity's status changed
        pass

    def _assert_state_blocked(self, flow: dict, assertion: dict) -> None:
        """Assert state transition was blocked."""
        # Check for error message or unchanged status
        pass

    def _assert_computed_value(self, assertion: dict, expected: Any) -> None:
        """Assert computed field has expected value."""
        pass

    def _assert_permission_granted(self, assertion: dict) -> None:
        """Assert permission was granted (no error)."""
        pass

    def _assert_permission_denied(self, assertion: dict) -> None:
        """Assert permission was denied (403 or error message)."""
        pass

    def _assert_ref_valid(self, assertion: dict) -> None:
        """Assert reference is valid."""
        pass

    def _assert_ref_invalid(self, assertion: dict) -> None:
        """Assert reference is invalid (error on create)."""
        pass

    def _track_coverage(self, flow: dict) -> None:
        """Track UX coverage from the flow."""
        entity = flow.get("entity", "")
        flow_type = self._get_flow_type(flow)

        if self.ux_tracker:
            if entity:
                self.ux_tracker.test_crud(entity, flow_type)
            for tag in flow.get("tags", []):
                if tag not in ["auto_generated", entity.lower()]:
                    pass  # Track tag

    def _take_screenshot(self, name: str) -> str:
        """Take a screenshot."""
        os.makedirs(SCREENSHOT_DIR, exist_ok=True)
        path = f"{SCREENSHOT_DIR}/{name}.png"
        try:
            self.page.screenshot(path=path)
        except Exception:
            pass
        return path


# =============================================================================
# Test Fixtures
# =============================================================================


@pytest.fixture(scope="module")
def api_client():
    """HTTP client for API operations."""
    return httpx.Client(base_url=DNR_BASE_URL, timeout=10)


@pytest.fixture(scope="module")
def generated_testspec():
    """Load or generate the E2ETestSpec for this example."""
    from dazzle.core.fileset import discover_dsl_files
    from dazzle.core.linker import build_appspec
    from dazzle.core.manifest import load_manifest
    from dazzle.core.parser import parse_modules
    from dazzle.testing.testspec_generator import generate_e2e_testspec

    # Find the example directory (mounted at /tests/e2e/docker, example at /app)
    example_paths = [
        Path(f"/app/examples/{EXAMPLE_NAME}"),
        Path(f"/examples/{EXAMPLE_NAME}"),
        Path(f"../../examples/{EXAMPLE_NAME}"),
    ]

    for example_path in example_paths:
        manifest_path = example_path / "dazzle.toml"
        if manifest_path.exists():
            try:
                manifest = load_manifest(manifest_path)
                dsl_files = discover_dsl_files(example_path, manifest)
                modules = parse_modules(dsl_files)
                appspec = build_appspec(modules, manifest.project_root)
                testspec = generate_e2e_testspec(appspec)

                # Convert to dict for easier handling
                flows = [f.model_dump() for f in testspec.flows]

                # Also load LLM-generated test designs from dsl/tests/designs.json
                designs_path = example_path / "dsl" / "tests" / "designs.json"
                if designs_path.exists():
                    try:
                        designs_data = json.loads(designs_path.read_text())
                        design_flows = _convert_designs_to_flows(designs_data)
                        flows.extend(design_flows)
                        print(f"Loaded {len(design_flows)} test designs from {designs_path}")
                    except Exception as e:
                        print(f"Warning: Could not load test designs: {e}")

                return {
                    "flows": flows,
                    "fixtures": [f.model_dump() for f in testspec.fixtures],
                    "metadata": testspec.metadata,
                }
            except Exception as e:
                print(f"Warning: Could not generate testspec: {e}")
                break

    # Return empty testspec if not found
    return {"flows": [], "fixtures": [], "metadata": {}}


def _convert_designs_to_flows(designs_data: list[dict]) -> list[dict]:
    """Convert TestDesignSpec objects to FlowSpec-compatible dicts."""
    flows = []
    for design in designs_data:
        # Only include accepted/implemented designs
        status = design.get("status", "proposed")
        if status not in ("accepted", "implemented", "verified"):
            continue

        # Convert test design steps to flow steps
        flow_steps = []
        for step in design.get("steps", []):
            flow_steps.append(
                {
                    "action": step.get("action", "click"),
                    "target": step.get("target", ""),
                    "data": step.get("data"),
                }
            )

        # Convert expected outcomes to assertions
        assertions = []
        for outcome in design.get("expected_outcomes", []):
            assertions.append(
                {
                    "type": "outcome",
                    "condition": outcome,
                }
            )

        flow = {
            "id": design.get("test_id", "unknown"),
            "description": design.get("title", ""),
            "type": _infer_flow_type(design),
            "entity": design.get("entities", [None])[0] if design.get("entities") else None,
            "steps": flow_steps,
            "assertions": assertions,
            "tags": design.get("tags", []) + ["test_design"],
            "persona": design.get("persona"),
            "scenario": design.get("scenario"),
        }
        flows.append(flow)

    return flows


def _infer_flow_type(design: dict) -> str:
    """Infer the flow type from a test design."""
    test_id = design.get("test_id", "").upper()
    tags = design.get("tags", [])

    if test_id.startswith("SM_") or "state_machine" in tags:
        return "state_machine"
    elif test_id.startswith("ACL_") or "access_control" in tags:
        return "access_control"
    elif test_id.startswith("CRUD_") or "crud" in tags:
        return "crud"
    elif test_id.startswith("SCENARIO_") or design.get("scenario"):
        return "scenario"
    elif design.get("persona") or "persona" in tags:
        return "persona"
    else:
        return "custom"


@pytest.fixture
def flow_executor(page: Page, api_client, ux_tracker):
    """Create a flow executor."""
    return FlowExecutor(page, api_client, ux_tracker)


# =============================================================================
# API Health Tests
# =============================================================================


class TestAPIHealth:
    """Test that the API is healthy before running flows."""

    def test_health_endpoint(self, api_client):
        """Test that the health endpoint returns success."""
        response = api_client.get("/health")
        assert response.status_code == 200
        data = response.json()
        assert data.get("status") == "healthy"

    def test_testspec_generated(self, generated_testspec):
        """Test that we have generated flows to execute."""
        flow_count = len(generated_testspec.get("flows", []))
        print(f"\nGenerated {flow_count} test flows for {EXAMPLE_NAME}")

        # Categorize flows
        flow_types: dict[str, int] = {}
        for flow in generated_testspec.get("flows", []):
            for tag in flow.get("tags", []):
                if tag in [
                    "crud",
                    "validation",
                    "navigation",
                    "state_machine",
                    "computed",
                    "access_control",
                    "reference",
                ]:
                    flow_types[tag] = flow_types.get(tag, 0) + 1
                    break

        if flow_types:
            print(f"Flow types: {flow_types}")


# =============================================================================
# Generated Flow Tests - By Category
# =============================================================================


class TestCRUDFlows:
    """Execute CRUD flows generated from DSL."""

    def test_crud_flows(
        self, page: Page, flow_executor: FlowExecutor, ux_tracker, generated_testspec
    ):
        """Run all CRUD flows."""
        crud_flows = [f for f in generated_testspec.get("flows", []) if "crud" in f.get("tags", [])]

        if not crud_flows:
            pytest.skip("No CRUD flows generated")

        results = []
        for flow in crud_flows[:10]:  # Limit to 10 for CI speed
            result = flow_executor.execute_flow(flow)
            results.append(result)

        passed = sum(1 for r in results if r.passed)
        total = len(results)
        print(f"\nCRUD Flows: {passed}/{total} passed")

        # At least 50% should pass
        assert passed >= total * 0.5, f"Too many CRUD flows failed: {passed}/{total}"


class TestValidationFlows:
    """Execute validation flows generated from field constraints."""

    def test_validation_flows(
        self, page: Page, flow_executor: FlowExecutor, ux_tracker, generated_testspec
    ):
        """Run all validation flows."""
        validation_flows = [
            f for f in generated_testspec.get("flows", []) if "validation" in f.get("tags", [])
        ]

        if not validation_flows:
            pytest.skip("No validation flows generated")

        results = []
        for flow in validation_flows[:5]:  # Limit for CI
            result = flow_executor.execute_flow(flow)
            results.append(result)

        passed = sum(1 for r in results if r.passed)
        print(f"\nValidation Flows: {passed}/{len(results)} passed")


class TestNavigationFlows:
    """Execute navigation flows for all surfaces."""

    def test_navigation_flows(
        self, page: Page, flow_executor: FlowExecutor, ux_tracker, generated_testspec
    ):
        """Run all navigation flows."""
        nav_flows = [
            f for f in generated_testspec.get("flows", []) if "navigation" in f.get("tags", [])
        ]

        if not nav_flows:
            pytest.skip("No navigation flows generated")

        results = []
        for flow in nav_flows:
            result = flow_executor.execute_flow(flow)
            results.append(result)

        passed = sum(1 for r in results if r.passed)
        print(f"\nNavigation Flows: {passed}/{len(results)} passed")


class TestStateMachineFlows:
    """Execute state machine transition flows."""

    def test_state_machine_flows(
        self, page: Page, flow_executor: FlowExecutor, ux_tracker, generated_testspec
    ):
        """Run state machine transition flows."""
        sm_flows = [
            f for f in generated_testspec.get("flows", []) if "state_machine" in f.get("tags", [])
        ]

        if not sm_flows:
            pytest.skip("No state machine flows generated")

        results = []
        for flow in sm_flows[:10]:  # Limit for CI
            result = flow_executor.execute_flow(flow)
            results.append(result)

        passed = sum(1 for r in results if r.passed)
        print(f"\nState Machine Flows: {passed}/{len(results)} passed")


class TestComputedFieldFlows:
    """Execute computed field verification flows."""

    def test_computed_field_flows(
        self, page: Page, flow_executor: FlowExecutor, ux_tracker, generated_testspec
    ):
        """Run computed field flows."""
        computed_flows = [
            f for f in generated_testspec.get("flows", []) if "computed" in f.get("tags", [])
        ]

        if not computed_flows:
            pytest.skip("No computed field flows generated")

        results = []
        for flow in computed_flows:
            result = flow_executor.execute_flow(flow)
            results.append(result)

        passed = sum(1 for r in results if r.passed)
        print(f"\nComputed Field Flows: {passed}/{len(results)} passed")


class TestAccessControlFlows:
    """Execute access control flows."""

    def test_access_control_flows(
        self, page: Page, flow_executor: FlowExecutor, ux_tracker, generated_testspec
    ):
        """Run access control flows."""
        access_flows = [
            f for f in generated_testspec.get("flows", []) if "access_control" in f.get("tags", [])
        ]

        if not access_flows:
            pytest.skip("No access control flows generated")

        results = []
        for flow in access_flows[:10]:  # Limit for CI
            result = flow_executor.execute_flow(flow)
            results.append(result)

        passed = sum(1 for r in results if r.passed)
        print(f"\nAccess Control Flows: {passed}/{len(results)} passed")


class TestReferenceFlows:
    """Execute reference integrity flows."""

    def test_reference_flows(
        self, page: Page, flow_executor: FlowExecutor, ux_tracker, generated_testspec
    ):
        """Run reference integrity flows."""
        ref_flows = [
            f for f in generated_testspec.get("flows", []) if "reference" in f.get("tags", [])
        ]

        if not ref_flows:
            pytest.skip("No reference flows generated")

        results = []
        for flow in ref_flows:
            result = flow_executor.execute_flow(flow)
            results.append(result)

        passed = sum(1 for r in results if r.passed)
        print(f"\nReference Flows: {passed}/{len(results)} passed")


class TestPersonaFlows:
    """Execute persona-based test designs (LLM-generated)."""

    def test_persona_flows(
        self, page: Page, flow_executor: FlowExecutor, ux_tracker, generated_testspec
    ):
        """Run persona flows from test designs."""
        persona_flows = [
            f
            for f in generated_testspec.get("flows", [])
            if f.get("type") == "persona" or f.get("persona")
        ]

        if not persona_flows:
            pytest.skip("No persona flows in test designs")

        results = []
        for flow in persona_flows:
            result = flow_executor.execute_flow(flow)
            results.append(result)

        passed = sum(1 for r in results if r.passed)
        print(f"\nPersona Flows (test_design): {passed}/{len(results)} passed")


class TestScenarioFlows:
    """Execute scenario-based test designs (LLM-generated)."""

    def test_scenario_flows(
        self, page: Page, flow_executor: FlowExecutor, ux_tracker, generated_testspec
    ):
        """Run scenario flows from test designs."""
        scenario_flows = [
            f
            for f in generated_testspec.get("flows", [])
            if f.get("type") == "scenario" or f.get("scenario")
        ]

        if not scenario_flows:
            pytest.skip("No scenario flows in test designs")

        results = []
        for flow in scenario_flows:
            result = flow_executor.execute_flow(flow)
            results.append(result)

        passed = sum(1 for r in results if r.passed)
        print(f"\nScenario Flows (test_design): {passed}/{len(results)} passed")


class TestCustomFlows:
    """Execute custom test designs (LLM-generated)."""

    def test_custom_flows(
        self, page: Page, flow_executor: FlowExecutor, ux_tracker, generated_testspec
    ):
        """Run custom flows from test designs."""
        custom_flows = [
            f
            for f in generated_testspec.get("flows", [])
            if f.get("type") == "custom" or "test_design" in f.get("tags", [])
        ]

        # Exclude flows already covered by other test classes
        custom_flows = [
            f
            for f in custom_flows
            if f.get("type") not in ("persona", "scenario", "state_machine", "access_control")
        ]

        if not custom_flows:
            pytest.skip("No custom flows in test designs")

        results = []
        for flow in custom_flows:
            result = flow_executor.execute_flow(flow)
            results.append(result)

        passed = sum(1 for r in results if r.passed)
        print(f"\nCustom Flows (test_design): {passed}/{len(results)} passed")


# =============================================================================
# Coverage Summary
# =============================================================================


class TestCoverageSummary:
    """Generate coverage summary at the end."""

    def test_write_coverage_report(self, ux_tracker, generated_testspec):
        """Write coverage report to file."""
        os.makedirs(SCREENSHOT_DIR, exist_ok=True)

        # Get flow statistics
        flows = generated_testspec.get("flows", [])
        flow_types: dict[str, int] = {}
        for flow in flows:
            # Check flow type first (for test designs)
            ftype = flow.get("type")
            if ftype in ["persona", "scenario", "custom"]:
                flow_types[ftype] = flow_types.get(ftype, 0) + 1
                continue

            # Fall back to tags (for deterministic flows)
            for tag in flow.get("tags", []):
                if tag in [
                    "crud",
                    "validation",
                    "navigation",
                    "state_machine",
                    "computed",
                    "access_control",
                    "reference",
                ]:
                    flow_types[tag] = flow_types.get(tag, 0) + 1
                    break

        coverage = {
            "example": EXAMPLE_NAME,
            "total_flows": len(flows),
            "flow_types": flow_types,
            "fixtures": len(generated_testspec.get("fixtures", [])),
            "metadata": generated_testspec.get("metadata", {}),
            "overall_coverage": min(100, len(flows) * 2),  # Rough estimate
        }

        # Write coverage report
        report_path = f"{SCREENSHOT_DIR}/ux_coverage.json"
        with open(report_path, "w") as f:
            json.dump(coverage, f, indent=2, default=str)

        print(f"\nCoverage report written to {report_path}")
        print(f"Total flows: {coverage['total_flows']}")
        print(f"Flow types: {coverage['flow_types']}")


# Mark all tests as e2e and docker
pytestmark = [pytest.mark.e2e, pytest.mark.docker]
