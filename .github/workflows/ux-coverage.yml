# UX Coverage Testing for Dazzle Examples
#
# This workflow runs E2E tests against each example project,
# captures screenshots, measures UX coverage, and generates
# promotional markdown showcasing the generated UIs.
#
# Artifacts:
#   - screenshots/<example>/  - UI screenshots
#   - coverage/<example>.json - Coverage metrics
#   - coverage/<example>.md   - Promotional markdown

name: UX Coverage

on:
  push:
    branches: [main]
    paths:
      - 'src/**'
      - 'examples/**'
      - 'tests/e2e/**'
  pull_request:
    branches: [main]
    paths:
      - 'src/**'
      - 'examples/**'
      - 'tests/e2e/**'
  workflow_dispatch:
    inputs:
      example:
        description: 'Specific example to test (leave empty for all)'
        required: false
        default: ''
      coverage_threshold:
        description: 'Minimum UX coverage percentage'
        required: false
        default: '50'

env:
  COVERAGE_THRESHOLD: ${{ github.event.inputs.coverage_threshold || '50' }}

jobs:
  # Discover which examples to test
  discover-examples:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - uses: actions/checkout@v6

      - name: Discover examples
        id: set-matrix
        run: |
          if [ -n "${{ github.event.inputs.example }}" ]; then
            # Single example specified
            echo "matrix={\"example\":[\"${{ github.event.inputs.example }}\"]}" >> $GITHUB_OUTPUT
          else
            # All examples (excluding _archive and other underscore-prefixed dirs)
            EXAMPLES=$(ls -d examples/*/ | xargs -I{} basename {} | grep -v '^_' | jq -R -s -c 'split("\n") | map(select(length > 0))')
            echo "matrix={\"example\":$EXAMPLES}" >> $GITHUB_OUTPUT
          fi

      - name: Show matrix
        run: |
          echo "Testing examples: ${{ steps.set-matrix.outputs.matrix }}"

  # Run UX coverage tests for each example
  ux-coverage:
    needs: discover-examples
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.discover-examples.outputs.matrix) }}

    steps:
      - uses: actions/checkout@v6

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Cache Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ matrix.example }}-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-${{ matrix.example }}-
            ${{ runner.os }}-buildx-

      - name: Create output directories
        run: |
          mkdir -p tests/e2e/docker/screenshots/${{ matrix.example }}
          mkdir -p tests/e2e/docker/coverage

      - name: Generate Docker Compose for ${{ matrix.example }}
        run: |
          cat > docker-compose.${{ matrix.example }}.yaml <<'EOF'
          # Auto-generated Docker Compose for E2E testing: ${{ matrix.example }}

          services:
            dnr-app:
              build:
                context: .
                dockerfile: tests/e2e/docker/Dockerfile.dnr
                args:
                  PROJECT_PATH: examples/${{ matrix.example }}
              container_name: dazzle-e2e-${{ matrix.example }}
              ports:
                - "8000:8000"
                - "3000:3000"
              environment:
                - DNR_TEST_MODE=1
                - DNR_HOST=0.0.0.0
                - DNR_API_PORT=8000
                - DNR_FRONTEND_PORT=3000
              healthcheck:
                test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
                interval: 5s
                timeout: 10s
                retries: 12
                start_period: 45s
              networks:
                - e2e-network

            playwright:
              image: mcr.microsoft.com/playwright/python:v1.55.0-noble
              container_name: dazzle-e2e-playwright-${{ matrix.example }}
              depends_on:
                dnr-app:
                  condition: service_healthy
              environment:
                - DNR_BASE_URL=http://dnr-app:8000
                - DNR_UI_URL=http://dnr-app:3000
                - SCREENSHOT_DIR=/screenshots
                - EXAMPLE_NAME=${{ matrix.example }}
                - PLAYWRIGHT_BROWSERS_PATH=/ms-playwright
              volumes:
                - ./tests:/tests:ro
                - ./src:/src:ro
                - ./examples:/examples:ro
                - ./tests/e2e/docker/screenshots/${{ matrix.example }}:/screenshots
              working_dir: /tests/e2e/docker
              networks:
                - e2e-network

          networks:
            e2e-network:
              driver: bridge
          EOF

      - name: Build DNR image
        run: |
          docker compose -f docker-compose.${{ matrix.example }}.yaml build dnr-app

      - name: Start services
        run: |
          docker compose -f docker-compose.${{ matrix.example }}.yaml up -d dnr-app

      - name: Wait for health check
        run: |
          echo "Waiting for DNR app to be healthy..."
          for i in {1..90}; do
            if docker compose -f docker-compose.${{ matrix.example }}.yaml ps | grep -q "healthy"; then
              echo "DNR app is healthy after ${i}s!"
              break
            fi
            if [ $i -eq 90 ]; then
              echo "Timeout waiting for DNR app"
              docker compose -f docker-compose.${{ matrix.example }}.yaml logs dnr-app
              exit 1
            fi
            sleep 1
          done

      - name: Run UX validation tests (v0.13.0 Generated Flows)
        id: run-tests
        run: |
          # Pin playwright version to match Docker image (v1.55.0-noble)
          # Run BOTH the generated flow tests AND the original validation tests
          docker compose -f docker-compose.${{ matrix.example }}.yaml run --rm playwright \
            bash -c "pip install httpx pytest-playwright 'playwright==1.55.0' /src --quiet && \
                     echo '=== Running Generated Flow Tests (v0.13.0) ===' && \
                     python -m pytest test_generated_flows.py -v --tb=short 2>&1 | tee test_output.txt || true && \
                     echo '' && \
                     echo '=== Running Original UX Validation Tests ===' && \
                     python -m pytest test_ux_validation.py -v --tb=short 2>&1 | tee -a test_output.txt" \
            || echo "test_exit_code=$?" >> $GITHUB_OUTPUT
          # Save test output for regression recording
          if [ -f test_output.txt ]; then
            cp test_output.txt "tests/e2e/docker/screenshots/${{ matrix.example }}/test_output.txt"
          fi

      - name: Record test regression (if failed)
        if: failure()
        continue-on-error: true
        run: |
          # Set up Python environment with dazzle
          pip install -e . --quiet

          # Parse test output for failures and record regressions
          cd examples/${{ matrix.example }}

          if [ -f "../../tests/e2e/docker/screenshots/${{ matrix.example }}/test_output.txt" ]; then
            # Extract failure info (simplified - captures first error)
            FAILURE_MSG=$(grep -A5 "FAILED\|ERROR" ../../tests/e2e/docker/screenshots/${{ matrix.example }}/test_output.txt | head -10 || echo "Test failed")

            python -m dazzle.cli test feedback record-regression \
              --test-id "E2E-${{ matrix.example }}-$(date +%Y%m%d)" \
              --test-path "tests/e2e/docker/test_ux_validation.py" \
              --message "$FAILURE_MSG" \
              --type assertion \
              --example "${{ matrix.example }}" \
              || echo "Warning: Could not record regression"
          fi

      - name: Copy coverage report
        if: always()
        run: |
          if [ -f "tests/e2e/docker/screenshots/${{ matrix.example }}/ux_coverage.json" ]; then
            cp "tests/e2e/docker/screenshots/${{ matrix.example }}/ux_coverage.json" \
               "tests/e2e/docker/coverage/${{ matrix.example }}.json"
          fi

      - name: Check coverage threshold
        id: check-coverage
        if: always()
        run: |
          COVERAGE_FILE="tests/e2e/docker/coverage/${{ matrix.example }}.json"
          if [ -f "$COVERAGE_FILE" ]; then
            OVERALL=$(python3 -c "import json; print(json.load(open('$COVERAGE_FILE')).get('overall_coverage', 0))")
            echo "coverage=$OVERALL" >> $GITHUB_OUTPUT
            echo "Overall UX Coverage for ${{ matrix.example }}: ${OVERALL}%"

            PASSES=$(python3 -c "print('yes' if $OVERALL >= ${{ env.COVERAGE_THRESHOLD }} else 'no')")
            if [ "$PASSES" = "no" ]; then
              echo "::error::Coverage ${OVERALL}% is below threshold ${{ env.COVERAGE_THRESHOLD }}%"
              echo "passed=false" >> $GITHUB_OUTPUT
            else
              echo "::notice::Coverage ${OVERALL}% meets threshold ${{ env.COVERAGE_THRESHOLD }}%"
              echo "passed=true" >> $GITHUB_OUTPUT
            fi
          else
            echo "::warning::No coverage report found for ${{ matrix.example }}"
            echo "coverage=0" >> $GITHUB_OUTPUT
            echo "passed=false" >> $GITHUB_OUTPUT
          fi

      - name: Generate promotional markdown
        if: always()
        run: |
          python3 tests/e2e/docker/generate_showcase_md.py \
            --example "${{ matrix.example }}" \
            --screenshots "tests/e2e/docker/screenshots/${{ matrix.example }}" \
            --coverage "tests/e2e/docker/coverage/${{ matrix.example }}.json" \
            --output "tests/e2e/docker/coverage/${{ matrix.example }}.md" \
            --project-root "." \
            || echo "Warning: Could not generate markdown"

      - name: Cleanup containers
        if: always()
        run: |
          docker compose -f docker-compose.${{ matrix.example }}.yaml down --volumes --remove-orphans || true

      - name: Upload screenshots
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: screenshots-${{ matrix.example }}
          path: tests/e2e/docker/screenshots/${{ matrix.example }}/
          retention-days: 30

      - name: Upload coverage report
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: coverage-${{ matrix.example }}
          path: |
            tests/e2e/docker/coverage/${{ matrix.example }}.json
            tests/e2e/docker/coverage/${{ matrix.example }}.md
          retention-days: 30

      # Note: Coverage threshold check is informational only during development
      # Remove continue-on-error once coverage improves across all examples
      - name: Report coverage status
        if: steps.check-coverage.outputs.passed == 'false'
        continue-on-error: true
        run: |
          echo "::warning::UX Coverage below threshold for ${{ matrix.example }} (coverage: ${{ steps.check-coverage.outputs.coverage }}%)"

  # Aggregate results and create combined report
  aggregate-results:
    needs: ux-coverage
    runs-on: ubuntu-latest
    if: always()

    steps:
      - uses: actions/checkout@v6

      - name: Download all coverage artifacts
        uses: actions/download-artifact@v6
        with:
          path: artifacts/
          pattern: coverage-*

      - name: Download all screenshot artifacts
        uses: actions/download-artifact@v6
        with:
          path: artifacts/
          pattern: screenshots-*

      - name: Generate combined report
        run: |
          mkdir -p combined-report

          # Aggregate coverage data
          python3 << 'EOF'
          import json
          import os
          from pathlib import Path

          coverage_data = {}
          artifact_dir = Path("artifacts")

          # Find all coverage JSON files
          for coverage_dir in artifact_dir.glob("coverage-*"):
              for json_file in coverage_dir.glob("*.json"):
                  example = json_file.stem
                  try:
                      with open(json_file) as f:
                          data = json.load(f)
                          coverage_data[example] = data.get("overall_coverage", 0)
                  except (json.JSONDecodeError, IOError) as e:
                      print(f"Warning: Could not read {json_file}: {e}")

          # Calculate overall stats
          if coverage_data:
              avg_coverage = sum(coverage_data.values()) / len(coverage_data)
              min_coverage = min(coverage_data.values())
              max_coverage = max(coverage_data.values())
          else:
              avg_coverage = min_coverage = max_coverage = 0

          # Generate summary
          summary = {
              "examples_tested": len(coverage_data),
              "average_coverage": round(avg_coverage, 1),
              "min_coverage": round(min_coverage, 1),
              "max_coverage": round(max_coverage, 1),
              "by_example": coverage_data,
              "threshold": int(os.environ.get("COVERAGE_THRESHOLD", 90)),
              "all_passing": all(v >= int(os.environ.get("COVERAGE_THRESHOLD", 90)) for v in coverage_data.values()) if coverage_data else False
          }

          with open("combined-report/summary.json", "w") as f:
              json.dump(summary, f, indent=2)

          # Generate markdown summary
          md = f"""# Dazzle UX Coverage Summary

          | Metric | Value |
          |--------|-------|
          | Examples Tested | {summary['examples_tested']} |
          | Average Coverage | {summary['average_coverage']}% |
          | Min Coverage | {summary['min_coverage']}% |
          | Max Coverage | {summary['max_coverage']}% |
          | Threshold | {summary['threshold']}% |
          | All Passing | {'Yes' if summary['all_passing'] else 'No'} |

          ## By Example

          | Example | Coverage | Status |
          |---------|----------|--------|
          """

          for example, coverage in sorted(coverage_data.items()):
              status = "Pass" if coverage >= summary['threshold'] else "**FAIL**"
              md += f"| {example} | {coverage}% | {status} |\n"

          md += "\n---\n*Generated by Dazzle UX Coverage workflow*\n"

          with open("combined-report/SUMMARY.md", "w") as f:
              f.write(md)

          print(f"Summary: {summary}")
          EOF

      - name: Upload combined report
        uses: actions/upload-artifact@v6
        with:
          name: ux-coverage-combined
          path: combined-report/
          retention-days: 90

      - name: Post summary to job
        run: |
          if [ -f "combined-report/SUMMARY.md" ]; then
            cat combined-report/SUMMARY.md >> $GITHUB_STEP_SUMMARY
          fi

      # Note: Coverage check is informational only during development
      # Remove continue-on-error once coverage improves across all examples
      - name: Report overall status
        continue-on-error: true
        run: |
          if [ -f "combined-report/summary.json" ]; then
            ALL_PASSING=$(python3 -c "import json; print(json.load(open('combined-report/summary.json')).get('all_passing', False))")
            AVG=$(python3 -c "import json; print(json.load(open('combined-report/summary.json')).get('average_coverage', 0))")
            if [ "$ALL_PASSING" = "False" ]; then
              echo "::warning::Not all examples meet the UX coverage threshold (average: ${AVG}%)"
            else
              echo "::notice::All examples meet the UX coverage threshold (average: ${AVG}%)"
            fi
          fi

  # Test Gap Analysis (v0.13.0)
  # Analyzes DSL specs to identify untested areas and suggest new tests
  gap-analysis:
    needs: [ux-coverage]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dazzle
        run: pip install -e .

      - name: Analyze test gaps for all examples
        id: gap-analysis
        run: |
          mkdir -p gap-reports

          # Analyze each example
          for example_dir in examples/*/; do
            example=$(basename "$example_dir")
            # Skip underscore-prefixed directories
            if [[ "$example" == _* ]]; then
              continue
            fi

            echo "Analyzing gaps for $example..."
            cd "$example_dir"

            if [ -f "dazzle.toml" ]; then
              # Run gap analysis using MCP-style output
              python3 << EOF > "../../gap-reports/${example}_gaps.json" 2>/dev/null || true
          import json
          import sys
          sys.path.insert(0, '../../src')

          from pathlib import Path
          from dazzle.core.fileset import discover_dsl_files
          from dazzle.core.linker import build_appspec
          from dazzle.core.manifest import load_manifest
          from dazzle.core.parser import parse_modules
          from dazzle.testing.testspec_generator import generate_e2e_testspec

          try:
              manifest_path = Path("dazzle.toml")
              manifest = load_manifest(manifest_path)
              dsl_files = discover_dsl_files(manifest_path.parent, manifest)
              modules = parse_modules(dsl_files)
              appspec = build_appspec(modules, manifest.project_root)

              testspec = generate_e2e_testspec(appspec)

              # Calculate coverage metrics
              entity_count = len(appspec.domain.entities)
              surface_count = len(appspec.surfaces)
              persona_count = len(appspec.personas) if appspec.personas else 0
              flow_count = len(testspec.flows)

              gaps = {
                  "example": "$example",
                  "entity_count": entity_count,
                  "surface_count": surface_count,
                  "persona_count": persona_count,
                  "generated_flows": flow_count,
                  "coverage_estimate": min(100, (flow_count / max(1, entity_count * 4)) * 100),
                  "suggestions": []
              }

              # Add suggestions based on what's missing
              if persona_count > 0 and flow_count < persona_count * 3:
                  gaps["suggestions"].append(f"Consider adding persona-specific tests for {persona_count} personas")

              if hasattr(appspec, 'state_machines') and appspec.state_machines:
                  sm_count = len(appspec.state_machines)
                  gaps["suggestions"].append(f"Found {sm_count} state machines - ensure transition tests exist")

              print(json.dumps(gaps, indent=2))
          except Exception as e:
              print(json.dumps({"example": "$example", "error": str(e)}))
          EOF
            fi
            cd ../..
          done

          # Aggregate gap reports
          python3 << 'EOF'
          import json
          from pathlib import Path

          gap_dir = Path("gap-reports")
          all_gaps = []

          for gap_file in gap_dir.glob("*_gaps.json"):
              try:
                  with open(gap_file) as f:
                      gaps = json.load(f)
                      if "error" not in gaps:
                          all_gaps.append(gaps)
              except:
                  pass

          summary = {
              "total_examples": len(all_gaps),
              "total_entities": sum(g.get("entity_count", 0) for g in all_gaps),
              "total_surfaces": sum(g.get("surface_count", 0) for g in all_gaps),
              "total_generated_flows": sum(g.get("generated_flows", 0) for g in all_gaps),
              "examples_with_suggestions": len([g for g in all_gaps if g.get("suggestions")]),
              "all_suggestions": []
          }

          for g in all_gaps:
              for s in g.get("suggestions", []):
                  summary["all_suggestions"].append(f"{g['example']}: {s}")

          with open("gap-reports/summary.json", "w") as f:
              json.dump(summary, f, indent=2)

          # Generate markdown
          md = f"""## Test Gap Analysis Summary

          | Metric | Value |
          |--------|-------|
          | Examples Analyzed | {summary['total_examples']} |
          | Total Entities | {summary['total_entities']} |
          | Total Surfaces | {summary['total_surfaces']} |
          | Generated Test Flows | {summary['total_generated_flows']} |
          | Examples with Suggestions | {summary['examples_with_suggestions']} |

          """

          if summary["all_suggestions"]:
              md += "\n### Suggested Improvements\n\n"
              for s in summary["all_suggestions"][:10]:  # Limit to 10
                  md += f"- {s}\n"

          with open("gap-reports/GAPS.md", "w") as f:
              f.write(md)
          EOF

      - name: Upload gap analysis
        uses: actions/upload-artifact@v6
        with:
          name: gap-analysis
          path: gap-reports/
          retention-days: 30

      - name: Post gap analysis to job summary
        run: |
          if [ -f "gap-reports/GAPS.md" ]; then
            cat gap-reports/GAPS.md >> $GITHUB_STEP_SUMMARY
          fi

      - name: Comment on PR with gap analysis
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        continue-on-error: true
        with:
          script: |
            const fs = require('fs');

            // Read gap analysis markdown
            let gapsContent = '';
            try {
              gapsContent = fs.readFileSync('gap-reports/GAPS.md', 'utf8');
            } catch (e) {
              console.log('No gap analysis to post');
              return;
            }

            // Find existing comment to update
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' && c.body.includes('Test Gap Analysis Summary')
            );

            const body = `<!-- dazzle-gap-analysis -->\n${gapsContent}`;

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }
